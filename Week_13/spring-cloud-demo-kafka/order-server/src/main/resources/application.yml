server: 
  port: 8082 
#  address: 0.0.0.0
#  servlet: 
#    context-path: /order-server

spring: 
  application:
    name: order-server
    
  datasource:
    url: jdbc:mysql://127.0.0.1:3306/order_db?useSSL=false&useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&serverTimezone=UTC
    username: root
    password: 
    # 8.x版本的MySQL JDBC驱动类为com.mysql.cj.jdbc.Driver
    # 5.X版本的MySQL JDBC驱动类为com.mysql.jdbc.Driver
    driver-class-name: com.mysql.jdbc.Driver
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      initial-size: 5
      min-idle: 5
      max-active: 20
      # 配置获取连接等待超时的时间
      max-wait: 60000    
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
      time-between-eviction-runs-millis: 60000  
      # 配置一个连接在池中最小生存的时间，单位是毫秒
      min-evictable-idle-time-millis: 300000  
      test-while-idle: true
      test-on-borrow:  false
      test-on-return: false
      ## 打开PSCache，并且指定每个连接上PSCache的大小
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
      filter: stat, wall, log4j
      use-global-data-source-stat:   true
      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
      connect-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500
      # 合并多个DruidDataSource的监控数据
      #spring.datasource.useGlobalDataSourceStat=true
      # druid 监控配置
      #StatViewServlet配置，说明请参考Druid Wiki，

#  redis:
#    database: 0
#    #    host: 10.10.7.172
#    #    port: 6379
#    password:
#    #集群模式
#    cluster:
#      nodes: 192.168.177.156:6380,192.168.177.156:6381,192.168.177.156:6382,192.168.177.156:6383,192.168.177.156:6384,192.168.177.156:6385
#      # 连接池最大连接数（使用负值表示没有限制）
#      max-active: 8
#      # 连接池中的最大空闲连接
#      max-idle: 8
#      # 连接池中的最小空闲连接
#      min-idle: 0
#      # 连接池最大阻塞等待时间（使用负值表示没有限制）
#      max-wait: -1
#    # 连接超时时间（毫秒）
#    timeout: 0
  kafka:
    bootstrap-servers:
    - 192.168.177.156:9092
    - 192.168.177.156:9093
    - 192.168.177.156:9094
    producer:
      # Kafka 生产者客户端Id,默认为"" 不设置为producer-1,producer-2
      client-id: order-kafka-producer
      # 序列化参数
      # org.apache.kafka.common.serialization.StringSerializer
      # org.apache.kafka.common.serialization.ByteArraySerializer
      # org.springframework.kafka.support.serializer.JsonSerializer
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 重新尝试次数
      retries: 0
      # 压缩格式
      compression-type: gzip
      # 批量发送的消息数量
      batch-size: 16384 
      # 32M的批处理缓冲区
      buffer-memory: 33554432 
       
    consumer:
      client-id: order-kafka-consumer
      group-id: order-kafka-consumer-group
      # 反序列化参数
      # org.apache.kafka.common.serialization.StringDeserializer
      # org.apache.kafka.common.serialization.ByteArrayDeserializer
      # org.springframework.kafka.support.serializer.JsonDeserializer
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 是否自动提交 关闭自动提交位移, 在消息被完整处理之后再手动提交位移.
      enable-auto-commit: false
      # 如何设置为自动提交（enable.auto.commit=true），这里设置自动提交周期
      # auto-commit-interval: 1000
      # 自动将偏移量置为最早的
      # earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 
      # latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 
      # none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
      auto-offset-reset: latest
      max-poll-records: 50
      properties:
        # 连接超时时间
        session-timeout-ms: 20000 
        # 手动提交设置与poll的心跳数,如果消息队列中没有消息，等待毫秒后，调用poll()方法。如果队列中有消息，立即消费消息，每次消费的消息的多少可以通过max.poll.records配置。
        max-poll-interval-ms: 15000 
        # 设置拉取数据的大小 15M
        max-partition-fetch-bytes: 15728640 
    listener: 
      ack-mode: manual



# 业务中需要操作的主体      
kafka:
   topic:
      order: hello-world-order 
      
      
      
# mybatis 基本配置
mybatis:
  # 配置Java类型包名
  type-aliases-package: com.gerry.pang.order.entity
  # 注册XML映射器
  mapper-locations: classpath:mapper/**/*.xml,classpath*:/com/gerry/pang/**/**mapper/*.xml
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 
    default-fetch-size: 20
    default-statement-timeout: 30       
  # 是否检测MyBatis运行参数配置文件
  check-config-location: true 
  # 指定MyBatis运行参数配置文件位置                            
  # config-location: classpath:/mybatis-config.xml
  # 配置类型处理器包名          
  type-handlers-package: com.gerry.pang.order.handlers
  # 指定执行器类型         
  executor-type: SIMPLE

# feign配置
feign:
  client:
    config:
      default:
        # 连接超时
        connectTimeout: 5000
        # 读取超时
        readTimeout: 5000
        # 日志等级
        loggerLevel: basic
  httpclient:
    enabled: true      
  okhttp:
    enabled: false
  hystrix:
    enabled: false
  # 压缩
  compression:
    # 请求
    request:
      # 是否开启
      enabled: true
      # 配置压缩数据大小的下限
      min-request-size: 2048
      # 开启支持压缩的MIME TYPE
      mime-types:
      - application/json
      - application/xml
      - text/xml
    # 响应
    response:
      enabled: true
      useGzipDecoder: true
      
      
# 线程池配置
async:
  executor:
    thread:
      # 核心线程池大小
      core-pool-size: 5
      # 最大线程数
      max-pool-size: 10
      # 队列容量
      queue-capacity: 100
      # 活跃时间
      keep-alive-seconds: 100
      # 线程名字前缀
      name-prefix: custom-thread-
      